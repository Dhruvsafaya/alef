package com.alefeducation.dimensions.generic_transform

import com.alefeducation.base.SparkBatchService
import com.alefeducation.bigdata.Sink
import com.alefeducation.schema.internal.ControlTableUtils.ProductMaxIdType
import com.alefeducation.service.DataSink
import com.alefeducation.util.BatchTransformerUtility._
import com.alefeducation.util.DataFrameUtility.getUTCDateFromMillisOrSeconds
import com.alefeducation.util.Resources._
import com.alefeducation.util.SparkSessionUtils
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.TimestampType
import org.apache.spark.sql.{DataFrame, SparkSession, functions => F}

/**
 * GenericDimIWHTransform class performs generic transformations on input DataFrames
 * based on configurations and writes the results to specified sinks.
 *
 * @param session      SparkSession instance.
 * @param service      SparkBatchService instance.
 * @param serviceName  Name of the service for configuration lookup.
 */
@deprecated("User GenericDimSCDTypeIITransformation")
class GenericDimSCDTransform(val session: SparkSession, val service: SparkBatchService, val serviceName: String) {

  /**
   * Explodes specified columns in the DataFrame.
   *
   * @param df       Input DataFrame.
   * @param columns  List of columns to be exploded.
   * @return         DataFrame with specified columns exploded.
   */
  private def explodeColumns(df: DataFrame, columns: List[String]): DataFrame = {
    columns.foldLeft(df) { (tempDf, colName) =>
      tempDf.withColumn(colName, explode_outer(col(colName)))
    }
  }

  /**
   * Converts specified columns from long epoch values to timestamp.
   *
   * @param df       Input DataFrame.
   * @param columns  List of columns to be converted.
   * @return         DataFrame with specified columns converted to timestamp.
   */
  private def longToTimestamp(df: DataFrame, columns: List[String]): DataFrame = {
    columns.foldLeft(df) { (tempDf, colName) =>
      tempDf.withColumn(colName, when(col(colName).isNotNull, getUTCDateFromMillisOrSeconds(col(colName)).cast(TimestampType)))
    }
  }

  /**
   * Performs transformations on the DataFrame based on configuration and writes the results to specified sinks.
   *
   * @return List of Sink instances with transformed data.
   */
  def transform(): List[Sink] = {
    val attachSource = getList(serviceName, "attach-source").headOption
    val updatedSource = getList(serviceName, "updated-source").headOption
    val detachSource = getList(serviceName, "detach-source").headOption
    val sinkName = getSink(serviceName).headOption

    val key = getNestedString(serviceName, "key")

    val attachEventNames = getList(serviceName, "attach-event-names")
    val unPlannedEventNames = getList(serviceName, "detach-event-name")
    val entityPrefix = getNestedString(serviceName, "entity-prefix")

    val cols = getNestedMap(serviceName, "colsMap")
    val attachColsMap = getNestedMap(serviceName, "attachColsMap")
    val detachColsMap = getNestedMap(serviceName, "detachColsMap")

    val colToTransform: Map[String, String] = getNestedMap(serviceName, "colToTransform")
    val colToExplode = getList(serviceName, "colToExplode")
    val colsFromLongToTimestamp = getList(serviceName, "longToTimeCols")

    val autoGenerateDwId = getBool(serviceName, "autoGenerateDwId")
    val inactiveStatusValue = getInt(serviceName, "inactiveStatus")

    val groupKeys = getList(serviceName, "groupKeys")

    val attachEvents = attachSource.flatMap(service.readOptional(_, session)).map(df => explodeColumns(df, colToExplode).selectColumnsWithMapping(attachColsMap))
    val updatedEvents = updatedSource.flatMap(service.readOptional(_, session)).map(df => explodeColumns(df, colToExplode).selectColumnsWithMapping(attachColsMap))
    val detachEvents = detachSource.flatMap(service.readOptional(_, session)).map(df => explodeColumns(df, colToExplode).selectColumnsWithMapping(detachColsMap))

    val mutatedAdt: Option[DataFrame] = attachEvents.unionOptionalByNameWithEmptyCheck(updatedEvents).unionOptionalByNameWithEmptyCheck(detachEvents)

    val withTransformedCols = mutatedAdt.map(df =>
      colToTransform.foldLeft(df) {
        case (tempDf, (colName, colValue)) =>
          tempDf.withColumn(colName, lit(colValue))
      })

    val withTimestampCols = withTransformedCols.map(df =>
      longToTimestamp(df, colsFromLongToTimestamp))

    val transformedIWH = withTimestampCols.map(
      _.transformForIWH2(
        cols,
        entityPrefix,
        1,
        attachEventNames,
        unPlannedEventNames,
        groupKey = groupKeys,
        inactiveStatus = inactiveStatusValue
      ))

    val sinkDf = if (autoGenerateDwId) {
      val startId = service.getStartIdUpdateStatus(key)
      transformedIWH
        .flatMap(_.genDwId(s"${entityPrefix}_dw_id", startId).checkEmptyDf)
    } else transformedIWH

    sinkDf.flatMap(s => sinkName.map(DataSink(_, s, controlTableUpdateOptions = Map(ProductMaxIdType -> getNestedString(serviceName, "key"))))).toList
  }
}

object GenericDimSCDTransform {
  def main(args: Array[String]): Unit = {
    val serviceName = args(0)
    val session: SparkSession = SparkSessionUtils.getSession(serviceName)
    val service = new SparkBatchService(serviceName, session)
    val transformer = new GenericDimSCDTransform(session, service, serviceName)
    service.runAll(transformer.transform())
  }
}
